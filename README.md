# Home_Sales
Created a new repository named **Home_Sales**.
Cloned the repository to the computer.
Renamed the Home_Sales_starter_code.ipynb file to **Home_Sales.ipynb**.
Imported the necessary PySpark SQL functions.
Read the home_sales_revised.csv data into a Spark DataFrame.
Created a temporary table called home_sales.
Wrote and executed SparkSQL queries to determine average home prices for various conditions.
Cached the temporary table home_sales and verified the cache.
Partitioned the home sales dataset by the date_built field and read the parquet data.
Uncached the home_sales temporary table and verified it was uncached.





